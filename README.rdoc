= HBase output plugin for Fluent event collector

== Overview

*HBase* output plugin buffers event logs in local file and puts it to HBase periodically.

== Installation

Simply use RubyGems:

    cp lib/fluent/plugin/out_hbasecsv.rb to /etc/td-agent/plugin

== Configuration

    <match pattern>
      type hbasecsv
  flush_interval 1
  include_tag_key true
  include_time_key true
  time_format %Y-%m-%d %H:%M:%S %:z
  fields_to_columns_mapping server_ip=>info:server_ip,logpath=>info:logpath,logvalue=>info:logvalue
  hbase_host  127.0.0.1
  hbase_port 9090
  errorlog_path /home/testlog/
  hbase_table weblog
    </match>

[include_time_key (optional)] If you need to write each event's time to HBase, set this true
and add a mapping from the 'time_key' to HBase column to the 'fields_to_columns_mapping' configuration.

The default is false.

[time_key (optional)] The key where each event's time is kept for mapping to a HBase column.

You MUST add a mapping from the key configured here to a HBase column,
to actually write times to HBase.

The default is "time".

[include_tag_key (optional)] If you need to write each event's tag to HBase, set this true
and add a mapping from the 'tag_key' to HBase column to the 'fields_to_columns_mapping' configuration.

The default is false.

[tag_key (optional)] The key where each event's tag is kept for mapping to a HBase column.

You MUST add a mapping from the key configured here to a HBase column,
to actually write tags to HBase.

The default is "tag".

[tag_column_name (deprecated)] The HBase column to save the tag attached to each Fleuntd event log,
in the format "[Column family name]:[Column name]".
For example, to save tags to the column "c" in the column family "cf", use "cf:c".

[time_column_name (deprecated)] The HBase column to save the time each Fluentd event log was sent,
in the format "[Column family name]:[Colum name]".
For example, to save the time to the column "t" in the column family "cf", use "cf:t".

[fields_to_columns_mapping (required)] The mapping from JSON fields to HBase columns,
in the format "[JSON_FIELD1]=>[HBASE_COLUMN1],[JSON_FIELD2]=>[HBASE_COLUMN2],...".

Each JSON_FIELD is formatted as field names separated by dot(.)s, e.g. "a.b.c".
Each HBASE_COLUMN is formatted as a tripled of a column family name, a colon(:), a column name, e.g. "cf:c".

[hbase_host (required)] HBase host

[hbase_port (required)] HBase port

[hbase_table (required)] HBase table name

[errorlog_path (required)] error format log output path

See example/fluent.conf for the configuration should work.

You can also test the configuration running a fluentd instance:

 fluentd -c example/fluent.conf --plugin lib/fluent/plugin

== Prerequiresites

You must setup your own Hadoop and HBase clusters and open appropriate ports to enable
the plugin to access HBase via HBase Thrift Server.

The plugin is tested solely on the system with:

- Hadoop 1.0.4
- HBase 0.94.0
- Java 1.6.0_37
- Mac OS X 10.8 Mountain Lion

for now.

Please let me know if you find the plugin to work in any other environments.

== Running

To make the plugin work, you need running instances of:

- Hadoop
- HBase
- HBase Thrift Server w/ the compact (buffered) protocol (not the framed protocol)

The procedure may be:

1. Start Hadoop

 $ start-all.sh

2. Start HBase

 $ start-hbase.sh

3. Start HBase Thrift Server with the compact protocol

Use the thread pool server as it is the only server supports the compact protocol:

 $ hbase thrift start -threadpool

4. Run Fluentd

== Copyright

Copyright:: Copyright (c) 2012 FURYU CORPORATION
License::   Apache License, Version 2.0
